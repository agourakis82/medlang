# Week 31: RL Environments & Policy Training - Implementation Summary

**Status**: ğŸš§ Core Infrastructure Complete (Partial Implementation)  
**Dependencies**: Week 29-30 (Surrogates & Evaluation), Week 28 (Contracts)

## What Was Implemented

### âœ… Core RL Abstractions (`src/rl/core.rs`)
- `State` - Vector-based state representation
- `Action` - Discrete action type (usize)
- `StepResult` - Transition outcome with reward, next_state, done, info
- `StepInfo` - Contract violation tracking + reward decomposition
- `Episode` - Trajectory tracking for analysis
- `RLEnv` trait - Standard interface for all RL environments

### âœ… State Discretization (`src/rl/discretizer.rs`)
- `StateDiscretizer` trait
- `BoxDiscretizer` - Uniform grid discretization for tabular Q-learning
- Handles multi-dimensional states with configurable bins per dimension
- 7 comprehensive unit tests

### âœ… DoseToxEnv QSP Environment (`src/rl/env_dose_tox.rs`)
- **State space**: [ANC, tumour_size, cycle, prev_dose] (normalized)
- **Action space**: Discrete dose levels (e.g., [0, 50, 100, 200, 300 mg])
- **Reward function**:
  ```
  reward = w_response * (tumour reduction)
         - w_tox * (ANC reduction)
         - contract_penalty * (# violations)
  ```
- **Contract integration**:
  - Grade 4 neutropenia (ANC < 0.25)
  - Critical neutropenia (ANC < 0.125) â†’ episode termination
  - Excessive dosing checks
- **Simplified QSP dynamics** with stochastic noise
- 10 comprehensive unit tests including determinism verification

## Remaining Work

### ğŸ”„ Q-Learning Trainer (`src/rl/train.rs`) - Not Implemented
**Would include**:
```rust
pub struct RLTrainConfig {
    pub n_episodes: usize,
    pub max_steps_per_episode: usize,
    pub gamma: f64,      // Discount factor
    pub alpha: f64,      // Learning rate
    pub eps_start: f64,  // Initial exploration
    pub eps_end: f64,    // Final exploration
}

pub struct RLTrainReport {
    pub n_episodes: usize,
    pub avg_reward: f64,
    pub final_epsilon: f64,
}

pub struct RLPolicyHandle {
    pub n_states: usize,
    pub n_actions: usize,
    pub q_values: Vec<f64>,  // Flattened Q-table
}

pub fn train_q_learning<E: RLEnv, D: StateDiscretizer>(
    env: &mut E,
    disc: &D,
    cfg: &RLTrainConfig,
) -> anyhow::Result<RLTrainReport>
```

**Algorithm**:
1. Îµ-greedy action selection
2. Q-value update: `Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]`
3. Îµ decay from eps_start to eps_end
4. Policy extraction via greedy selection

### ğŸ”„ Standard Library Types (`stdlib/med/rl.medlang`) - Not Implemented
```medlang
module med.rl;

import med.ml.backend::{BackendKind};

type RLEnvConfig = {
  evidence_program: EvidenceProgram;
  backend: BackendKind;
  n_cycles: Int;
  dose_levels: Vector<Real>;
  w_response: Real;
  w_tox: Real;
  contract_penalty: Real;
};

type RLTrainConfig = {
  n_episodes: Int;
  max_steps_per_episode: Int;
  gamma: Real;
  alpha: Real;
  eps_start: Real;
  eps_end: Real;
};

type RLTrainReport = {
  n_episodes: Int;
  avg_reward: Real;
  final_epsilon: Real;
};

type PolicyEvalReport = {
  n_episodes: Int;
  avg_reward: Real;
  avg_contract_violations: Real;
};

type RLPolicy = opaque;  // Policy handle
```

### ğŸ”„ Built-in Functions - Not Implemented
**Type Signatures**:
```rust
train_policy_rl(
    env_cfg: RLEnvConfig,
    train_cfg: RLTrainConfig
) -> RLTrainReport

simulate_policy_rl(
    env_cfg: RLEnvConfig,
    policy: RLPolicy,
    n_episodes: Int
) -> PolicyEvalReport
```

### ğŸ”„ CLI Commands - Not Implemented
```bash
mlc train-policy-rl \
  --evidence models/oncology.medlang:OncologyEvidence \
  --env-config env_cfg.json \
  --train-config train_cfg.json \
  --out-policy policy.json \
  --out-report train_report.json

mlc eval-policy-rl \
  --evidence models/oncology.medlang:OncologyEvidence \
  --env-config env_cfg.json \
  --policy policy.json \
  --n-episodes 500 \
  --out-report eval_report.json
```

## Design & Architecture

### RL Environment Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DoseToxEnv (implements RLEnv)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Configuration:                         â”‚
â”‚ - Evidence program handle              â”‚
â”‚ - Backend (Mechanistic/Surrogate)      â”‚
â”‚ - n_cycles horizon                     â”‚
â”‚ - dose_levels_mg action space          â”‚
â”‚ - Reward weights                       â”‚
â”‚                                        â”‚
â”‚ State: [ANC, tumour, cycle, prev_dose]â”‚
â”‚ Action: dose level index (discrete)    â”‚
â”‚                                        â”‚
â”‚ Dynamics:                              â”‚
â”‚ 1. Map action â†’ dose_mg                â”‚
â”‚ 2. Simulate QSP/surrogate 1 cycle      â”‚
â”‚ 3. Update ANC, tumour from simulation  â”‚
â”‚ 4. Check contracts â†’ violations        â”‚
â”‚ 5. Compute reward                      â”‚
â”‚ 6. Check termination                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reward Decomposition

```
Total Reward = efficacy - toxicity - contracts

where:
  efficacy = w_response * (prev_tumour - new_tumour)
  toxicity = w_tox * max(0, prev_anc - new_anc)
  contracts = contract_penalty * num_violations
```

### Contract Integration

Contracts from Week 28 become **safety monitors** in RL:

1. **Observation**: Contract violations are counted per step
2. **Penalty**: Each violation adds `-contract_penalty` to reward
3. **Termination**: Critical violations (e.g., ANC < 0.125) end episode early
4. **Policy Learning**: Agent learns to avoid actions that violate contracts

This creates **safe RL** where policy naturally avoids dangerous doses.

### Q-Learning Algorithm (Design)

```
Initialize Q(s,a) = 0 for all s,a
for episode = 1 to n_episodes:
    s = env.reset()
    for step = 1 to max_steps:
        # Îµ-greedy action selection
        if random() < Îµ:
            a = random_action()
        else:
            a = argmax_a Q(s,a)
        
        # Take step
        s', r, done, info = env.step(a)
        
        # Q-learning update
        target = r + Î³ * max_a' Q(s',a')
        Q(s,a) â† Q(s,a) + Î± * (target - Q(s,a))
        
        s = s'
        if done: break
    
    # Decay exploration
    Îµ â† Îµ * decay_rate

# Extract policy: Ï€(s) = argmax_a Q(s,a)
```

## Usage Examples (Intended)

### Example 1: Train RL Policy

```medlang
module projects.oncology_rl_dosing;

import med.rl::{RLEnvConfig, RLTrainConfig, RLTrainReport};
import med.ml.backend::{BackendKind};
import med.oncology.evidence::{OncologyEvidence};

fn main() -> RLTrainReport {
  let ev: EvidenceProgram = OncologyEvidence;

  let env_cfg: RLEnvConfig = {
    evidence_program = ev;
    backend = BackendKind::Surrogate;  // Fast training
    n_cycles = 6;
    dose_levels = [0.0, 50.0, 100.0, 200.0, 300.0];
    w_response = 1.0;
    w_tox = 2.0;
    contract_penalty = 10.0;
  };

  let train_cfg: RLTrainConfig = {
    n_episodes = 1000;
    max_steps_per_episode = 6;
    gamma = 0.95;
    alpha = 0.1;
    eps_start = 0.5;
    eps_end = 0.05;
  };

  train_policy_rl(env_cfg, train_cfg)
}
```

### Example 2: Evaluate Learned Policy

```medlang
fn evaluate_policy(policy: RLPolicy) -> PolicyEvalReport {
  let env_cfg: RLEnvConfig = {
    evidence_program = OncologyEvidence;
    backend = BackendKind::Mechanistic;  // Evaluate on true model
    n_cycles = 6;
    dose_levels = [0.0, 50.0, 100.0, 200.0, 300.0];
    w_response = 1.0;
    w_tox = 2.0;
    contract_penalty = 10.0;
  };

  simulate_policy_rl(env_cfg, policy, 500)
}
```

## Key Innovations

### 1. QSP-Native RL
Unlike generic RL libraries, DoseToxEnv is **built for pharmacometrics**:
- State space includes clinical biomarkers (ANC, tumour size)
- Actions are doses (mg), not abstract integers
- Rewards balance efficacy vs. toxicity (not just "win/lose")
- Dynamics come from mechanistic QSP models

### 2. Contract-Aware Safety
Week 28 contracts become **safety constraints** in RL:
- Violations add penalties â†’ agent learns to avoid them
- Critical violations terminate episodes â†’ hard safety bounds
- Policy qualification includes violation counts

### 3. Surrogate-Accelerated Training
Uses Week 29-30 infrastructure:
- Train on fast surrogates (100x speedup)
- Evaluate on mechanistic models (ground truth)
- Leverages `BackendKind` abstraction

### 4. First-Class RL in DSL
RL is not a Python script calling MedLangâ€”it's **in the language**:
```medlang
let policy: RLPolicy = train_policy_rl(env_cfg, train_cfg);
let report: PolicyEvalReport = simulate_policy_rl(env_cfg, policy, 500);

if report.avg_contract_violations == 0 && report.avg_reward > threshold {
  deploy_policy(policy);
}
```

## Testing Status

### âœ… Completed Tests
- **Core RL**: 4 tests (State, StepResult, Episode tracking, StepInfo)
- **Discretizer**: 7 tests (uniform/non-uniform bins, clamping, boundaries)
- **DoseToxEnv**: 10 tests (reset, step, episodes, contracts, determinism)

**Total**: 21 tests, all passing

### ğŸ”„ Remaining Tests Needed
- Q-learning convergence on toy MDP
- Policy training on DoseToxEnv
- Built-in function integration
- CLI smoke tests

## Performance Considerations

### Training Time
With surrogate backend:
- 1 episode â‰ˆ 6 steps Ã— 10 ms = 60 ms
- 1000 episodes â‰ˆ 60 seconds
- Practical for hyperparameter tuning

With mechanistic backend:
- 1 episode â‰ˆ 6 steps Ã— 1 second = 6 seconds
- 1000 episodes â‰ˆ 1.7 hours
- Use for final evaluation only

### State Space
- Continuous: 4D vector (infinite states)
- Discretized (10 bins/dim): 10^4 = 10,000 states
- Q-table size: 10,000 states Ã— 5 actions Ã— 8 bytes = 400 KB (tiny!)

## Scientific Impact

Week 31 enables **agent-based dose optimization**:

**Traditional Approach**:
1. Design fixed dose schedule (e.g., 200 mg every 3 weeks)
2. Run Phase I/II trials
3. High toxicity â†’ reduce dose, repeat

**MedLang RL Approach**:
1. Train RL policy on QSP model with surrogates
2. Policy learns: "If ANC < 0.5, reduce dose; if tumour growing, increase dose"
3. Evaluate policy on mechanistic model
4. **Adaptive dosing** that balances efficacy and safety

This is the foundation for **precision dosing** where treatment adapts to individual patient response.

## Comparison with Other Systems

### Python (Stable-Baselines3)
```python
from stable_baselines3 import DQN

# Generic environment, no QSP integration
env = gym.make("CartPole-v1")
model = DQN("MlpPolicy", env)
model.learn(total_timesteps=10000)
```

**MedLang advantage**:
- âœ“ QSP-native environments
- âœ“ Contract integration
- âœ“ Type-safe RL configuration
- âœ— Python has richer RL ecosystem

### OpenAI Gym Custom Env
```python
class DoseEnv(gym.Env):
    def step(self, action):
        # Manual QSP integration
        ...
```

**MedLang advantage**:
- âœ“ First-class in language
- âœ“ Surrogate backend integration
- âœ“ Contract violation tracking built-in
- âœ— Less flexibility for research

## Future Work (Post-Week 31)

1. **Deep RL**: DQN, A3C, PPO for high-dimensional states
2. **Multi-Agent RL**: Multiple treatments, combination therapy
3. **Offline RL**: Learn from clinical trial data
4. **Safe RL**: Formal safety constraints (CPO, TRPO with constraints)
5. **Meta-RL**: Transfer policies across similar drugs
6. **Model-Based RL**: Plan using QSP model as world model
7. **Inverse RL**: Infer reward from expert (clinician) behavior

## Conclusion

Week 31 adds **reinforcement learning** to MedLang's capabilities:

**Before Week 31**: Train surrogates, evaluate quality, run evidence programs

**After Week 31**: **Learn policies** that make treatment decisions, balancing efficacy and safety

The core infrastructure is complete:
- âœ… RLEnv abstraction
- âœ… DoseToxEnv QSP environment with contracts
- âœ… State discretization for tabular RL
- âœ… 21 comprehensive tests

Remaining work (Q-learning trainer, built-ins, CLI) follows established patterns from Weeks 29-30 and can be completed following the design outlined here.

**MedLang now has the foundation for agent-based precision medicine.**

---

**Implementation Note**: This represents ~40% implementation of Week 31. The core abstractions and environment are production-ready. The trainer and language integration would require an additional ~500 lines following the patterns established in previous weeks.
