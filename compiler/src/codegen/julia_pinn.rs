//! Julia + DiffEqFlux codegen for neural surrogate / PINN backends
//!
//! Generates a Julia script that trains a neural surrogate of the mechanistic
//! PBPK-QSP model using Flux.jl and optionally DiffEqFlux for physics-informed training.

use crate::ir::surrogate::IRSurrogateConfig;
use crate::ir::IRModel;

/// Generate Julia + DiffEqFlux training script for neural surrogate
pub fn generate_julia_pinn(_ir_model: &IRModel, cfg: &IRSurrogateConfig) -> String {
    let mut code = String::new();

    // Header and imports
    code.push_str(&generate_header());
    code.push_str("\n\n");

    // Data loading function
    code.push_str(&generate_data_loader(cfg));
    code.push_str("\n\n");

    // Model architecture
    code.push_str(&generate_model_architecture(cfg));
    code.push_str("\n\n");

    // Training function
    code.push_str(&generate_training_function(cfg));
    code.push_str("\n\n");

    // Save/load functions
    code.push_str(&generate_save_load());
    code.push_str("\n\n");

    // Prediction wrapper
    code.push_str(&generate_prediction_wrapper(cfg));
    code.push_str("\n\n");

    // Physics loss scaffold (if enabled)
    if cfg.use_physics_loss {
        code.push_str(&generate_physics_loss_scaffold());
        code.push_str("\n\n");
    }

    // Main entry point
    code.push_str(&generate_main());

    code
}

fn generate_header() -> String {
    r#"# Neural Surrogate Training Script for MedLang Model
# Generated by MedLang Compiler - Week 15: PINN Backend
#
# This script trains a neural network surrogate of the mechanistic
# PBPK-QSP model using Flux.jl and optionally DiffEqFlux.jl

using Pkg
# Ensure required packages (uncomment if needed):
# Pkg.add(["Flux", "DiffEqFlux", "DifferentialEquations", "Optim", "CSV", "DataFrames", "BSON"])

using Flux
using DiffEqFlux
using DifferentialEquations
using Optim
using CSV
using DataFrames
using BSON: @save, @load
using Printf"#
        .to_string()
}

fn generate_data_loader(cfg: &IRSurrogateConfig) -> String {
    let feature_names = cfg.input_features.join(", ");
    let target_names = cfg.output_observables.join(", ");

    format!(
        r#""""
    load_training_data(path::AbstractString)

Load training data from CSV file with columns:
Input features: {}
Output targets: {}
"""
function load_training_data(path::AbstractString)
    df = CSV.read(path, DataFrame)

    # Input features: {}
    X = Array(hcat(
        df.TIME,
        df.DOSE_MG,
        df.WT,
        df.Kd_QM,
        df.Kp_QM,
    ))'

    # Target: {}
    y = Array(df.TUMVOL)'

    @info "Loaded training data" size(X) size(y)

    return X, y
end"#,
        feature_names, target_names, feature_names, target_names
    )
}

fn generate_model_architecture(cfg: &IRSurrogateConfig) -> String {
    let input_dim = cfg.input_dim();
    let output_dim = cfg.output_dim();

    // Build layer specification string
    let mut layers = Vec::new();
    layers.push(format!("Dense(input_dim, {}, relu)", cfg.hidden_layers[0]));

    for i in 1..cfg.hidden_layers.len() {
        layers.push(format!(
            "Dense({}, {}, relu)",
            cfg.hidden_layers[i - 1],
            cfg.hidden_layers[i]
        ));
    }

    let last_hidden = cfg.hidden_layers.last().unwrap();
    layers.push(format!("Dense({}, output_dim)", last_hidden));

    let layers_str = layers.join(",\n        ");

    format!(
        r#""""
    build_surrogate(input_dim::Int, output_dim::Int)

Construct neural network architecture for the surrogate model.
Input: {} features
Output: {} observables
Hidden layers: {:?}
"""
function build_surrogate(input_dim::Int, output_dim::Int)
    return Chain(
        {}
    )
end"#,
        input_dim, output_dim, cfg.hidden_layers, layers_str
    )
}

fn generate_training_function(cfg: &IRSurrogateConfig) -> String {
    let use_physics = if cfg.use_physics_loss {
        r#"
    # Physics-informed loss (if enabled)
    physics_weight = 0.1
    total_loss(x, y) = data_loss(x, y) + physics_weight * physics_loss(model, x)"#
    } else {
        "\n    total_loss(x, y) = data_loss(x, y)"
    };

    format!(
        r#""""
    train_surrogate(data_path::AbstractString; n_epochs::Int=100, η::Float64=1e-3)

Train the neural surrogate model on provided training data.

# Arguments
- `data_path`: Path to CSV training data
- `n_epochs`: Number of training epochs (default: 100)
- `η`: Learning rate (default: 1e-3)

# Returns
- Trained Flux model
"""
function train_surrogate(data_path::AbstractString; n_epochs::Int=100, η::Float64=1e-3)
    X, y = load_training_data(data_path)

    input_dim  = size(X, 1)
    output_dim = size(y, 1)

    @info "Building surrogate model" input_dim output_dim
    model = build_surrogate(input_dim, output_dim)
    ps    = Flux.params(model)

    # Data-driven loss
    data_loss(x, y_true) = Flux.Losses.mse(model(x), y_true)
    {}

    opt = ADAM(η)

    @info "Starting training..." n_epochs

    best_loss = Inf
    for epoch in 1:n_epochs
        gs = gradient(() -> total_loss(X, y), ps)
        Flux.Optimise.update!(opt, ps, gs)

        current_loss = data_loss(X, y)

        if epoch % 10 == 0
            @printf "Epoch %4d: loss = %.6f\n" epoch current_loss
        end

        if current_loss < best_loss
            best_loss = current_loss
        end
    end

    @info "Training complete" final_loss=best_loss

    return model
end"#,
        use_physics
    )
}

fn generate_save_load() -> String {
    r#""""
    save_surrogate(model, path::AbstractString)

Save trained surrogate model to BSON file.
"""
function save_surrogate(model, path::AbstractString)
    @save path model
    @info "Saved surrogate model to $path"
end

"""
    load_surrogate(path::AbstractString)

Load trained surrogate model from BSON file.
"""
function load_surrogate(path::AbstractString)
    @load path model
    @info "Loaded surrogate model from $path"
    return model
end"#
        .to_string()
}

fn generate_prediction_wrapper(cfg: &IRSurrogateConfig) -> String {
    let feature_list = cfg.input_features.join(", ");

    format!(
        r#""""
    predict_tumour(model, times::Vector{{Float64}}, dose_mg, WT, Kd_QM, Kp_QM)

Predict tumour volume trajectory using trained surrogate.

# Arguments
- `model`: Trained Flux model
- `times`: Vector of time points (days)
- `dose_mg`: Dose in mg
- `WT`: Patient weight in kg
- `Kd_QM`: Quantum-derived Kd (M)
- `Kp_QM`: Quantum-derived Kp (tumor/plasma)

# Returns
- Vector of predicted tumour volumes at specified times
"""
function predict_tumour(model, times::Vector{{Float64}}, dose_mg, WT, Kd_QM, Kp_QM)
    n = length(times)

    # Construct input feature matrix: [{}]
    X = Array(hcat(
        times,
        fill(dose_mg, n),
        fill(WT, n),
        fill(Kd_QM, n),
        fill(Kp_QM, n),
    ))'

    # Predict
    y_pred = model(X)

    return vec(y_pred)
end"#,
        feature_list
    )
}

fn generate_physics_loss_scaffold() -> String {
    r#"# ============================================================================
# Physics-Informed Loss (PINN) Scaffold
# ============================================================================

"""
    physics_loss(model, X_batch)

Compute physics-informed loss term based on known ODE structure.

This is a scaffold for Week 15. Full implementation would:
1. Compute neural derivative d/dt model(X) numerically
2. Compare to mechanistic ODE right-hand side
3. Penalize discrepancies

# Arguments
- `model`: Neural network
- `X_batch`: Batch of input features [time; dose; WT; Kd_QM; Kp_QM]

# Returns
- Physics-based regularization loss
"""
function physics_loss(model, X_batch)
    # TODO: Implement physics-informed loss
    # Example structure:
    # 1. For each sample in batch:
    #    - Compute dT/dt from model (finite differences)
    #    - Compute expected dT/dt from mechanistic ODE
    #    - Accumulate squared difference
    #
    # For now, return zero (pure data-driven training)
    return 0.0
end

# Future: Add mechanistic ODE RHS for comparison
# function mechanistic_tumor_derivative(T, C_tumor, params)
#     # Example: dT/dt = kg * T * (1 - T/Tmax) - kkill * C_tumor * T
#     return ...
# end"#
        .to_string()
}

fn generate_main() -> String {
    r#"# ============================================================================
# Main Entry Point
# ============================================================================

"""
    main()

Main training workflow. Expects command-line arguments:
1. Training data CSV path
2. Output model path (.bson)
3. (Optional) Number of epochs
"""
function main()
    if length(ARGS) < 2
        println("Usage: julia surrogate_train.jl <train.csv> <out.bson> [n_epochs]")
        println()
        println("Example:")
        println("  julia surrogate_train.jl data/oncology_training.csv models/surrogate.bson 200")
        println()
        println("Training data CSV should have columns:")
        println("  TIME, DOSE_MG, WT, Kd_QM, Kp_QM, TUMVOL")
        return
    end

    data_path = ARGS[1]
    out_path  = ARGS[2]
    n_epochs  = length(ARGS) >= 3 ? parse(Int, ARGS[3]) : 100

    @info "Training neural surrogate" data_path out_path n_epochs

    model = train_surrogate(data_path; n_epochs=n_epochs)
    save_surrogate(model, out_path)

    println()
    println("✓ Training complete! Model saved to: $out_path")
    println()
    println("To use the trained surrogate:")
    println("  julia> using BSON: @load")
    println("  julia> @load \"$out_path\" model")
    println("  julia> times = [0.0, 7.0, 14.0, 21.0, 28.0]")
    println("  julia> preds = predict_tumour(model, times, 200.0, 70.0, 1e-9, 0.5)")
end

# Run main if script is executed directly
if abspath(PROGRAM_FILE) == @__FILE__
    main()
end"#
        .to_string()
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::ir::IRModel;

    #[test]
    fn test_generate_julia_pinn_basic() {
        let cfg = IRSurrogateConfig::default_oncology_qsp();
        let ir_model = IRModel {
            name: "TestModel".to_string(),
            states: vec![],
            params: vec![],
            inputs: vec![],
            random_effects: vec![],
            intermediates: vec![],
            odes: vec![],
            observables: vec![],
            individual_params: vec![],
        };

        let code = generate_julia_pinn(&ir_model, &cfg);

        // Check for key components
        assert!(code.contains("using Flux"));
        assert!(code.contains("using DiffEqFlux"));
        assert!(code.contains("function load_training_data"));
        assert!(code.contains("function build_surrogate"));
        assert!(code.contains("function train_surrogate"));
        assert!(code.contains("function predict_tumour"));
        assert!(code.contains("function main()"));
    }

    #[test]
    fn test_generate_with_physics_loss() {
        let mut cfg = IRSurrogateConfig::default_oncology_qsp();
        cfg.use_physics_loss = true;

        let ir_model = IRModel {
            name: "TestModel".to_string(),
            states: vec![],
            params: vec![],
            inputs: vec![],
            random_effects: vec![],
            intermediates: vec![],
            odes: vec![],
            observables: vec![],
            individual_params: vec![],
        };

        let code = generate_julia_pinn(&ir_model, &cfg);

        // Should include physics loss scaffold
        assert!(code.contains("function physics_loss"));
        assert!(code.contains("physics_weight"));
    }

    #[test]
    fn test_layer_generation() {
        let mut cfg = IRSurrogateConfig::default_oncology_qsp();
        cfg.hidden_layers = vec![32, 64, 32];

        let ir_model = IRModel {
            name: "TestModel".to_string(),
            states: vec![],
            params: vec![],
            inputs: vec![],
            random_effects: vec![],
            intermediates: vec![],
            odes: vec![],
            observables: vec![],
            individual_params: vec![],
        };

        let code = generate_julia_pinn(&ir_model, &cfg);

        // Should have correct number of layers
        assert!(code.contains("Dense(input_dim, 32, relu)"));
        assert!(code.contains("Dense(32, 64, relu)"));
        assert!(code.contains("Dense(64, 32, relu)"));
        assert!(code.contains("Dense(32, output_dim)"));
    }
}
